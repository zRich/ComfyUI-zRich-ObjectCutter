import json
import numpy as np
from PIL import Image
import torch

class ZRichObjectCutter:
    """
    ğŸ§© ZRich Object Cutter
    å°†å›¾ç‰‡ä¸­çš„ç‰©ä½“ï¼ˆé€šè¿‡ MASKï¼‰æŠ å›¾ä¸ºä¸åŸå›¾åŒå°ºå¯¸çš„é€æ˜å›¾åƒã€‚
    ç°åœ¨æ”¯æŒç›´æ¥æ¥æ”¶ SAM2 çš„ MASK è¾“å‡ºï¼Œé€ä¸ªåŒºåŸŸç”Ÿæˆé€æ˜èƒŒæ™¯ RGBA å›¾ç‰‡ã€‚
    å¯é€‰åœ°æ¥æ”¶ bboxesï¼Œç”¨æ©ç åˆæˆåçš„å›¾æŒ‰æ¡†é€ä¸ªè£å‰ªè¾“å‡ºã€‚
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
                # ç›´æ¥æ¥æ”¶ Sam2Segmentation çš„è¾“å‡º MASK
                "mask": ("MASK",),
                # æ”¹ä¸ºå¿…å¡«ï¼Œä¸ Florence2toCoordinates çš„ BBOX è¾“å‡ºä¸€è‡´
                "bboxes": ("BBOX",),
            },
        }

    RETURN_TYPES = ("IMAGE", "IMAGE")
    RETURN_NAMES = ("images", "crops")
    FUNCTION = "cut_objects"
    CATEGORY = "zRich/Segmentation"

    # åŸºäº MASK æŠ å›¾å¹¶è¿”å›é€æ˜ RGBA å›¾åƒï¼ˆå°ºå¯¸ä¸åŸå›¾ä¸€è‡´ï¼‰
    def cut_objects(self, image, mask, bboxes):
        # è¾“å…¥ image: (B, H, W, C) æµ®ç‚¹ 0..1ï¼›mask: (..., H, W)
        img_np = image.detach().cpu().numpy().astype(np.float32)  # ä¿æŒ 0..1
        B, H, W, C = img_np.shape

        mask_np = mask.detach().cpu().numpy().astype(np.float32)

        # ç»Ÿä¸€ä¸º (N, H, W)ï¼šå°†é™¤æœ€åä¸¤ä¸ªç»´åº¦å¤–çš„æ‰€æœ‰å‰å¯¼ç»´åº¦å±•å¹³ä¸º N
        if mask_np.ndim == 2:
            masks_np = mask_np.reshape(1, H, W)
        elif mask_np.ndim >= 3:
            h, w = mask_np.shape[-2], mask_np.shape[-1]
            n = int(np.prod(mask_np.shape[:-2]))
            masks_np = mask_np.reshape(n, h, w)
        else:
            masks_np = np.zeros((0, H, W), dtype=np.float32)

        outputs = []
        # å§‹ç»ˆä½¿ç”¨ç¬¬ä¸€å¼ åŸå›¾è¿›è¡ŒæŠ å›¾ï¼›å¯¹æ¯ä¸ª mask è¾“å‡ºä¸€å¼ é€æ˜å›¾
        src = img_np[0]  # (H, W, C)

        # å¦‚æœæä¾›äº† bboxesï¼Œåˆ™ä¾æ®æ¯ä¸ªæ¡†æŠŠåŸå§‹æ©ç â€œæ‹†åˆ†â€ä¸ºå¤šä¸ªå…¨å°ºå¯¸æ©ç ï¼šä»…ä¿ç•™æ¡†å†…åƒç´ 
        # æ„å»ºå¹¶é›†æ©ç ï¼Œä½œä¸ºæ‹†åˆ†çš„åŸºåº•
        union_alpha_for_split = np.zeros((H, W), dtype=np.float32)
        if masks_np.shape[0] > 0:
            for i in range(masks_np.shape[0]):
                union_alpha_for_split = np.maximum(union_alpha_for_split, (masks_np[i] > 0.5).astype(np.float32))

        # è§£æ bboxesï¼šæ”¯æŒ [ [x1,y1,x2,y2], ... ] æˆ–æŒ‰æ‰¹æ¬¡åµŒå¥—ç»“æ„
        boxes_for_split = []
        try:
            if isinstance(bboxes, torch.Tensor):
                bb = bboxes.detach().cpu().numpy()
            else:
                bb = np.array(bboxes, dtype=np.int64)
            if bb.ndim == 1 and bb.shape[0] == 4:
                boxes_for_split = [bb.tolist()]
            elif bb.ndim >= 2:
                reshaped = bb.reshape(-1, bb.shape[-1])
                if reshaped.shape[-1] == 4:
                    boxes_for_split = reshaped.tolist()
        except Exception:
            boxes_for_split = []

        # è‹¥æœ‰æ¡†ï¼Œåˆ™æŒ‰æ¡†ç”Ÿæˆå…¨å°ºå¯¸æ©ç ï¼ˆæ©ç ä¸åŸå›¾å°ºå¯¸ä¸€è‡´ï¼Œä»…ä¿ç•™æ¡†å†…åƒç´ ï¼‰ï¼Œå¹¶æ’é™¤äº¤å‰åŒºåŸŸ
        # è§„åˆ™ï¼šæŒ‰æ¡†é¡ºåºèµ‹äºˆåƒç´ å½’å±ï¼ŒåŒä¸€åƒç´ åªå½’å±äºç¬¬ä¸€ä¸ªè¦†ç›–å®ƒçš„æ¡†
        if len(boxes_for_split) > 0:
            def clamp_int(v, lo, hi):
                return int(max(lo, min(hi, v)))
            split_masks = []
            assigned = np.zeros((H, W), dtype=np.uint8)  # å·²åˆ†é…åƒç´ æ ‡è®°ï¼ˆ0/1ï¼‰
            union_alpha_for_split_bin = (union_alpha_for_split > 0.5).astype(np.uint8)
            for bx in boxes_for_split:
                x1, y1, x2, y2 = bx
                x1 = clamp_int(x1, 0, W)
                x2 = clamp_int(x2, 0, W)
                y1 = clamp_int(y1, 0, H)
                y2 = clamp_int(y2, 0, H)
                if x2 <= x1 or y2 <= y1:
                    continue
                # å€™é€‰åƒç´ ï¼šå¹¶é›†æ©ç ä¸­çš„åƒç´ 
                candidate = union_alpha_for_split_bin[y1:y2, x1:x2]
                # æ’é™¤å·²åˆ†é…åƒç´ ï¼Œç¡®ä¿ä¸äº§ç”Ÿäº¤å‰
                exclusive = candidate * (1 - assigned[y1:y2, x1:x2])
                # å°†æœ¬æ¡†çš„ç‹¬å åƒç´ å†™å…¥å…¨å°ºå¯¸æ©ç 
                full_mask = np.zeros((H, W), dtype=np.float32)
                full_mask[y1:y2, x1:x2] = exclusive.astype(np.float32)
                split_masks.append(full_mask)
                # æ ‡è®°è¿™äº›åƒç´ ä¸ºå·²åˆ†é…
                assigned[y1:y2, x1:x2] = np.maximum(assigned[y1:y2, x1:x2], exclusive)
            if len(split_masks) > 0:
                masks_np = np.stack(split_masks, axis=0)
        for i in range(masks_np.shape[0]):
            m = masks_np[i]

            # äºŒå€¼åŒ–æ©ç å¹¶åšé€æ˜èƒŒæ™¯ï¼ˆæŒ‰ 0/1 é˜ˆå€¼ï¼‰
            alpha = (m > 0.5).astype(np.float32)
            rgb = src * alpha[..., None]
            rgba = np.concatenate([rgb, alpha[..., None]], axis=-1)

            outputs.append(torch.from_numpy(rgba).unsqueeze(0))  # (1, H, W, 4)

        if len(outputs) == 0:
            # å¦‚æœæ²¡æœ‰æ©ç ï¼Œè¾“å‡ºä¸€å¼ é€æ˜å›¾ï¼ˆä¸ç¬¬ä¸€å¼ åŸå›¾åŒå°ºå¯¸ï¼‰
            blank = np.zeros((H, W, 4), dtype=np.float32)
            blank_t = torch.from_numpy(blank).unsqueeze(0)
            return (blank_t, blank_t)

        # ç¬¬ä¸€è·¯è¾“å‡ºï¼šæ¯ä¸ª mask çš„æ•´å¹…é€æ˜æŠ å›¾
        per_mask_rgba = torch.cat(outputs, dim=0)  # (N, H, W, 4)

        # ç¬¬äºŒè·¯è¾“å‡ºï¼šå¦‚æœæä¾›äº† bboxesï¼Œåˆ™æŒ‰æ¡†è¾“å‡ºåªåŒ…å«è¯¥æ¡†ç‹¬å åƒç´ çš„å…¨å°ºå¯¸é€æ˜å›¾
        crop_outputs = []
        def clamp_int(v, lo, hi):
            return int(max(lo, min(hi, v)))

        # ç›´æ¥å¤ç”¨ boxes_for_splitï¼Œç¡®ä¿ä¸ masks_np çš„é¡ºåºä¸€è‡´
        boxes = boxes_for_split

        if boxes:
            for i, bx in enumerate(boxes):
                x1, y1, x2, y2 = bx
                # è¾¹ç•Œè£å‰ªå¹¶ä¿è¯æœ‰æ•ˆåŒºåŸŸ
                x1 = clamp_int(x1, 0, W)
                x2 = clamp_int(x2, 0, W)
                y1 = clamp_int(y1, 0, H)
                y2 = clamp_int(y2, 0, H)
                if x2 <= x1 or y2 <= y1:
                    continue
                # ä½¿ç”¨æ¯ä¸ªæ¡†å¯¹åº”çš„ç‹¬å æ©ç ç”Ÿæˆ RGBAï¼Œå¹¶ä»…åœ¨æ¡†åŒºåŸŸæ‹·è´
                alpha_i = (masks_np[i] > 0.5).astype(np.float32)
                rgba_i = np.concatenate([src * alpha_i[..., None], alpha_i[..., None]], axis=-1)
                canvas = np.zeros((H, W, 4), dtype=np.float32)
                canvas[y1:y2, x1:x2, :] = rgba_i[y1:y2, x1:x2, :].astype(np.float32)
                crop_outputs.append(torch.from_numpy(canvas).unsqueeze(0))

        if crop_outputs:
            crops_batch = torch.cat(crop_outputs, dim=0)
        else:
            # æ²¡æœ‰æä¾› bboxes æˆ–è§£æå¤±è´¥æ—¶ï¼Œå¤ç”¨æ•´å¹…æŠ å›¾ï¼ˆç¬¬ä¸€è·¯ï¼‰
            crops_batch = per_mask_rgba

        return (per_mask_rgba, crops_batch)
