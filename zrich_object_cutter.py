import json
import numpy as np
from PIL import Image
import torch

class ZRichObjectCutter:
    """
    ğŸ§© ZRich Object Cutter
    å°†å›¾ç‰‡ä¸­çš„ç‰©ä½“ï¼ˆé€šè¿‡ MASKï¼‰æŠ å›¾ä¸ºä¸åŸå›¾åŒå°ºå¯¸çš„é€æ˜å›¾åƒã€‚
    ç°åœ¨æ”¯æŒç›´æ¥æ¥æ”¶ SAM2 çš„ MASK è¾“å‡ºï¼Œé€ä¸ªåŒºåŸŸç”Ÿæˆé€æ˜èƒŒæ™¯ RGBA å›¾ç‰‡ã€‚
    å¯é€‰åœ°æ¥æ”¶ bboxesï¼Œç”¨æ©ç åˆæˆåçš„å›¾æŒ‰æ¡†é€ä¸ªè£å‰ªè¾“å‡ºã€‚
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
                # ç›´æ¥æ¥æ”¶ Sam2Segmentation çš„è¾“å‡º MASK
                "mask": ("MASK",),
                # æ”¹ä¸ºå¿…å¡«ï¼Œä¸ Florence2toCoordinates çš„ BBOX è¾“å‡ºä¸€è‡´
                "bboxes": ("BBOX",),
            },
        }

    RETURN_TYPES = ("IMAGE", "IMAGE")
    RETURN_NAMES = ("images", "crops")
    FUNCTION = "cut_objects"
    CATEGORY = "zRich/Segmentation"

    # åŸºäº MASK æŠ å›¾å¹¶è¿”å›é€æ˜ RGBA å›¾åƒï¼ˆå°ºå¯¸ä¸åŸå›¾ä¸€è‡´ï¼‰
    def cut_objects(self, image, mask, bboxes):
        # è¾“å…¥ image: (B, H, W, C) æµ®ç‚¹ 0..1ï¼›mask: (..., H, W)
        img_np = image.detach().cpu().numpy().astype(np.float32)  # ä¿æŒ 0..1
        B, H, W, C = img_np.shape

        mask_np = mask.detach().cpu().numpy().astype(np.float32)

        # ç»Ÿä¸€ä¸º (N, H, W)ï¼šå°†é™¤æœ€åä¸¤ä¸ªç»´åº¦å¤–çš„æ‰€æœ‰å‰å¯¼ç»´åº¦å±•å¹³ä¸º N
        if mask_np.ndim == 2:
            masks_np = mask_np.reshape(1, H, W)
        elif mask_np.ndim >= 3:
            h, w = mask_np.shape[-2], mask_np.shape[-1]
            n = int(np.prod(mask_np.shape[:-2]))
            masks_np = mask_np.reshape(n, h, w)
        else:
            masks_np = np.zeros((0, H, W), dtype=np.float32)

        outputs = []
        # å§‹ç»ˆä½¿ç”¨ç¬¬ä¸€å¼ åŸå›¾è¿›è¡ŒæŠ å›¾ï¼›å¯¹æ¯ä¸ª mask è¾“å‡ºä¸€å¼ é€æ˜å›¾
        src = img_np[0]  # (H, W, C)
        for i in range(masks_np.shape[0]):
            m = masks_np[i]

            # äºŒå€¼åŒ–æ©ç å¹¶åšé€æ˜èƒŒæ™¯ï¼ˆæŒ‰ 0/1 é˜ˆå€¼ï¼‰
            alpha = (m > 0.5).astype(np.float32)
            rgb = src * alpha[..., None]
            rgba = np.concatenate([rgb, alpha[..., None]], axis=-1)

            outputs.append(torch.from_numpy(rgba).unsqueeze(0))  # (1, H, W, 4)

        if len(outputs) == 0:
            # å¦‚æœæ²¡æœ‰æ©ç ï¼Œè¾“å‡ºä¸€å¼ é€æ˜å›¾ï¼ˆä¸ç¬¬ä¸€å¼ åŸå›¾åŒå°ºå¯¸ï¼‰
            blank = np.zeros((H, W, 4), dtype=np.float32)
            blank_t = torch.from_numpy(blank).unsqueeze(0)
            return (blank_t, blank_t)

        # ç¬¬ä¸€è·¯è¾“å‡ºï¼šæ¯ä¸ª mask çš„æ•´å¹…é€æ˜æŠ å›¾
        per_mask_rgba = torch.cat(outputs, dim=0)  # (N, H, W, 4)

        # ç¬¬äºŒè·¯è¾“å‡ºï¼šå¦‚æœæä¾›äº† bboxesï¼Œåˆ™æŒ‰æ¡†ä»åˆæˆå›¾è£å‰ªï¼›å¦åˆ™å¤ç”¨ç¬¬ä¸€è·¯
        # åˆæˆå›¾ï¼šæ©ç å¹¶é›†åçš„æ•´ä½“æŠ å›¾ï¼ˆé¿å…å¤šå¯¹è±¡è¢«è¦†ç›–ä¸ºé»‘ï¼‰
        union_alpha = np.zeros((H, W), dtype=np.float32)
        for i in range(masks_np.shape[0]):
            union_alpha = np.maximum(union_alpha, (masks_np[i] > 0.5).astype(np.float32))
        union_rgb = src * union_alpha[..., None]
        union_rgba = np.concatenate([union_rgb, union_alpha[..., None]], axis=-1)  # (H,W,4)

        crop_outputs = []
        def clamp_int(v, lo, hi):
            return int(max(lo, min(hi, v)))

        # è§£æ bboxesï¼šæ”¯æŒ [ [x1,y1,x2,y2], ... ] æˆ–æŒ‰æ‰¹æ¬¡åµŒå¥—ç»“æ„
        boxes = []
        try:
            # torch/numpy/list ç»Ÿä¸€ä¸º Python åˆ—è¡¨
            if isinstance(bboxes, torch.Tensor):
                bb = bboxes.detach().cpu().numpy()
            else:
                bb = np.array(bboxes, dtype=np.int64)
            # å°è¯•å±•å¹³åˆ° (M,4)
            if bb.ndim == 1 and bb.shape[0] == 4:
                boxes = [bb.tolist()]
            elif bb.ndim >= 2:
                # å¦‚æœæ˜¯æŒ‰æ‰¹æ¬¡åµŒå¥—ï¼Œåˆ™å–ç¬¬ä¸€ç»´çš„æ‰€æœ‰æ¡†æˆ–ç›´æ¥é‡å¡‘åˆ° (-1,4)
                reshaped = bb.reshape(-1, bb.shape[-1])
                if reshaped.shape[-1] == 4:
                    boxes = reshaped.tolist()
        except Exception:
            boxes = []

        if boxes:
            for bx in boxes:
                x1, y1, x2, y2 = bx
                # è¾¹ç•Œè£å‰ªå¹¶ä¿è¯æœ‰æ•ˆåŒºåŸŸ
                x1 = clamp_int(x1, 0, W)
                x2 = clamp_int(x2, 0, W)
                y1 = clamp_int(y1, 0, H)
                y2 = clamp_int(y2, 0, H)
                if x2 <= x1 or y2 <= y1:
                    continue
                # æ¯ä¸ª bbox è¾“å‡ºä¸åŸå›¾åŒå°ºå¯¸çš„é€æ˜å›¾ï¼Œåªåœ¨æ¡†åŒºåŸŸæ‹·è´åƒç´ 
                canvas = np.zeros((H, W, 4), dtype=np.float32)
                canvas[y1:y2, x1:x2, :] = union_rgba[y1:y2, x1:x2, :].astype(np.float32)
                crop_outputs.append(torch.from_numpy(canvas).unsqueeze(0))

        if crop_outputs:
            crops_batch = torch.cat(crop_outputs, dim=0)
        else:
            # æ²¡æœ‰æä¾› bboxes æˆ–è§£æå¤±è´¥æ—¶ï¼Œå¤ç”¨æ•´å¹…æŠ å›¾ï¼ˆç¬¬ä¸€è·¯ï¼‰
            crops_batch = per_mask_rgba

        return (per_mask_rgba, crops_batch)
