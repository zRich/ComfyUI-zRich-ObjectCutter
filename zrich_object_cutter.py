import json
import numpy as np
from PIL import Image
import torch

class ZRichObjectCutter:
    """
    ğŸ§© ZRich Object Cutter
    å°†å›¾ç‰‡ä¸­çš„ç‰©ä½“ï¼ˆé€šè¿‡ bbox å®šä½ï¼‰è£å‰ªæˆä¸åŸå›¾åŒå°ºå¯¸çš„é€æ˜å›¾åƒã€‚
    å¯ç›´æ¥è¾“å‡ºç»™ Preview Image / Save Image èŠ‚ç‚¹ã€‚
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
                # ä¸ Florence2Run çš„ data ç±»å‹ä¸€è‡´ï¼Œæ”¯æŒç›´æ¥ä¼ å…¥å…¶ JSON è¾“å‡º
                "data": ("JSON",),
            },
            "optional": {
                "padding": ("FLOAT", {
                    "default": 0.0,
                    "min": 0.0,
                    "max": 0.2,
                    "step": 0.01
                }),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("images",)
    FUNCTION = "cut_objects"
    CATEGORY = "zRich/Segmentation"

    # è£å‰ªç‰©ä½“å¹¶è¿”å›é€æ˜å›¾åƒ
    def cut_objects(self, image, data, padding=0.0):
        # Convert image tensor â†’ PIL
        image_np = (image[0].cpu().numpy() * 255).astype(np.uint8)
        img_pil = Image.fromarray(image_np)
        width, height = img_pil.size

        images = []

        # å…¼å®¹ Florence2Run çš„ JSON è¾“å‡ºä¸ Florence2toCoordinates çš„è¾“å…¥æ ¼å¼
        # data å¯èƒ½æ˜¯ï¼š
        # - JSON å­—ç¬¦ä¸²ï¼ˆåŒ…å« list æˆ– dictï¼‰
        # - Python åˆ—è¡¨ï¼ˆ[[x1,y1,x2,y2], ...] æˆ– [ {"bboxes": [...]}, ... ]ï¼‰
        # - Python å­—å…¸ï¼ˆ{"bboxes": [...] }ï¼‰
        parsed = data
        if isinstance(parsed, str):
            try:
                parsed = json.loads(parsed.replace("'", '"'))
            except Exception:
                # å¦‚æœä¸æ˜¯æœ‰æ•ˆ JSONï¼Œåˆ™å½“ä½œç©ºå¤„ç†
                parsed = []

        # æå– bboxes åˆ—è¡¨
        bboxes = []
        if isinstance(parsed, dict) and "bboxes" in parsed:
            bboxes = parsed["bboxes"]
        elif isinstance(parsed, list):
            # å¦‚æœæ˜¯åˆ—è¡¨ä¸”ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ dictï¼Œåˆ™å–å…¶ä¸­çš„ bboxesï¼ˆå…¼å®¹ batch æƒ…å†µï¼Œé»˜è®¤å–ç¬¬ä¸€ä¸ªï¼‰
            if len(parsed) > 0 and isinstance(parsed[0], dict) and "bboxes" in parsed[0]:
                bboxes = parsed[0]["bboxes"]
            else:
                # å¦åˆ™å‡è®¾å°±æ˜¯ [[x1,y1,x2,y2], ...]
                bboxes = parsed
        else:
            bboxes = []

        for i, box in enumerate(bboxes):
            x1, y1, x2, y2 = [float(v) for v in box]

            # Add padding
            pad_w = (x2 - x1) * padding
            pad_h = (y2 - y1) * padding
            x1 = max(0, int(x1 - pad_w))
            y1 = max(0, int(y1 - pad_h))
            x2 = min(width, int(x2 + pad_w))
            y2 = min(height, int(y2 + pad_h))

            # Transparent canvas same size as original
            transparent = Image.new("RGBA", img_pil.size, (0, 0, 0, 0))
            crop = img_pil.crop((x1, y1, x2, y2)).convert("RGBA")
            transparent.paste(crop, (x1, y1))

            # Convert back to tensor
            np_img = np.array(transparent).astype(np.float32) / 255.0
            tensor_img = torch.from_numpy(np_img)[None,]
            images.append(tensor_img)

        if len(images) == 0:
            blank = Image.new("RGBA", img_pil.size, (0, 0, 0, 0))
            np_blank = np.array(blank).astype(np.float32) / 255.0
            return (torch.from_numpy(np_blank)[None,],)

        result = torch.cat(images, dim=0)
        return (result,)
